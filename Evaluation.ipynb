{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d909c8-2143-4c7a-9590-1d174a5422c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import time\n",
    "\n",
    "# adjust PyTorch parameter to enable more efficient use of GPU memory\n",
    "import os \n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:native, garbage_collection_threshold:0.6, max_split_size_mb:64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32400a6-863e-4a32-8bb2-8b5d352e9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Modules.Models.UNets as UNets\n",
    "import Modules.Data.DICHeLaDataset as DICHeLaSegDataset \n",
    "import Modules.Data.ImageStackTransform as ImageStackTransform  \n",
    "import Modules.Utils.Evaluations as Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de36f45-aaea-409c-8215-b8d3e7b0f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalidate_data_file_path_globs = [\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01\\t*.tif\",\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02\\t*.tif\"\n",
    "]\n",
    "\n",
    "trainvalidate_seg_file_path_globs = [\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_ST\\SEG_ERODE\\man_seg*.tif\",\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_ST\\SEG_ERODE\\man_seg*.tif\"\n",
    "]\n",
    "\n",
    "src_model_path = r\".\\Results\\model_2024-07-07-18-50-42.pt\"\n",
    "# src_model_path = r\".\\Results\\model_2024-07-07-11-16-47.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfec313-dd6e-4f07-a213-202f245345aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define data transforms\n",
    "importlib.reload(ImageStackTransform)\n",
    "\n",
    "# create common transforms\n",
    "common_transform = None # no need to perform common transforms\n",
    "\n",
    "data_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float, scale = False),\n",
    "    v2.Resize(size = 512, antialias=True,),\n",
    "])\n",
    "\n",
    "target_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.long, scale = False),\n",
    "    v2.Resize(size = 512, antialias=True,),\n",
    "    v2.Lambda(lambda x: torch.squeeze(x, dim = 0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d479e76-3a70-414b-8142-3a4a944e46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data set\n",
    "importlib.reload(DICHeLaSegDataset)\n",
    "\n",
    "color_categories = False\n",
    "\n",
    "trainvalidate_dataset = DICHeLaSegDataset.DICHeLaSegDataset(\n",
    "    data_image_path_globs = trainvalidate_data_file_path_globs,\n",
    "    seg_image_path_globs = trainvalidate_seg_file_path_globs,\n",
    "    data_transform = data_transform,\n",
    "    target_transform = target_transform,\n",
    "    common_transform = common_transform,\n",
    "    color_categories = color_categories,\n",
    ")\n",
    "\n",
    "print(f\"Tot data size = {len(trainvalidate_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732dc28d-f01f-4a45-945e-402e675383e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train and validate dataset \n",
    "data_split_rand_genenrator = torch.Generator().manual_seed(0)\n",
    "data_split_ratios = [0.8, 0.2]\n",
    "\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(\n",
    "    trainvalidate_dataset, \n",
    "    data_split_ratios, \n",
    "    generator = data_split_rand_genenrator)\n",
    "\n",
    "print(f\"Train data size = {len(train_dataset)}\")\n",
    "print(f\"Validate data size = {len(validate_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ad379-14a1-4d45-9003-0f3e3894cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check data and label\n",
    "check_idx = 0\n",
    "\n",
    "check_dataset = validate_dataset\n",
    "\n",
    "check_data, check_label = check_dataset[check_idx]\n",
    "print(check_data.size())\n",
    "print(check_label.size())\n",
    "\n",
    "check_data = check_data.numpy()\n",
    "check_label = check_label.numpy()\n",
    "\n",
    "plt.figure(figsize = (7,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.rollaxis(check_data,0,3))\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Data\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(check_label, cmap = \"Set3\")\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Target\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b19606-47a3-4337-b3e4-76f90393d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data loader to training and validation dataset\n",
    "\n",
    "# NOTE: Use a very small batch size here to fit the data into my small GPU memory \n",
    "train_bath_size = 16\n",
    "validate_batch_size = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size = train_bath_size, \n",
    "                                               shuffle = False)\n",
    "validate_dataloader = torch.utils.data.DataLoader(validate_dataset, \n",
    "                                               batch_size = validate_batch_size, \n",
    "                                               shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d6c47-6b57-448c-894a-f90d5e1ce1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "\n",
    "model = torch.load(src_model_path)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d285c-d133-401d-93ac-d23af6fad9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use parallel computing if possible\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6620869b-9f1c-4aad-baf4-e24c2d9d1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## interate through dataset and evalution model in terms of IOU\n",
    "importlib.reload(Evaluations)\n",
    "\n",
    "src_dataloader = validate_dataloader\n",
    "bkg_val = 0\n",
    "\n",
    "mean_ious = Evaluations.mean_iou_over_dataset(\n",
    "    model = model,\n",
    "    src_dataloader = src_dataloader,\n",
    "    device = device,\n",
    "    bkg_val = bkg_val,\n",
    ")\n",
    "\n",
    "print(\"IOUs{class:val} :\")\n",
    "print(mean_ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0e671-cec9-4bb2-aa51-31a2c999c1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
