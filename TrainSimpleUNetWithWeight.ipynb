{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f91c0c-67f7-4a55-ab86-6e1c699e90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import time\n",
    "\n",
    "# adjust PyTorch parameter to enable more efficient use of GPU memory\n",
    "import os \n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:native, garbage_collection_threshold:0.6, max_split_size_mb:64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d043d-099a-4ce9-94d6-2c181ef64a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Modules.Models.UNets as UNets\n",
    "import Modules.Data.DICHeLaDataset as DICHeLaSegDataset \n",
    "import Modules.Data.ImageStackTransform as ImageStackTransform  \n",
    "import Modules.TrainAndValidate.TrainAndValidate as TrainAndValidate\n",
    "import Modules.TrainAndValidate.LossFunctions as LossFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483cf24-74c7-419c-8dba-7340ca7ddf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalidate_data_file_path_globs = [\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01\\t*.tif\",\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02\\t*.tif\"\n",
    "]\n",
    "\n",
    "trainvalidate_seg_file_path_globs = [\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_ST\\SEG_ERODE\\man_seg*.tif\",\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_ST\\SEG_ERODE\\man_seg*.tif\"\n",
    "]\n",
    "\n",
    "\n",
    "trainvalidate_weight_file_path_globs = [\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_ST\\SEG_WEIGHT\\man_seg*.tif\",\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_ST\\SEG_WEIGHT\\man_seg*.tif\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a28a7-64a7-4dca-a873-f1335b5cb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define data transforms\n",
    "importlib.reload(ImageStackTransform)\n",
    "\n",
    "# create common transforms\n",
    "common_transform = v2.Compose([\n",
    "    ImageStackTransform.ElasticTransform(fills = [\"mean\", \"min\", \"min\"], alpha = 50, sigma = 5),\n",
    "    ImageStackTransform.RandomRotation(fills = [\"mean\", \"min\", \"min\"], degrees = [-45, 45]),\n",
    "    ImageStackTransform.RandomCrop(\n",
    "        size = (256,256), \n",
    "        pad_if_needed = True, \n",
    "        padding_mode = \"reflect\",\n",
    "        \n",
    "    ),\n",
    "    ImageStackTransform.RandomHorizontalFlip(p = 0.5),\n",
    "    ImageStackTransform.RandomVerticalFlip(p = 0.5),\n",
    "    \n",
    "])\n",
    "\n",
    "## NOTE: scaling and normalization is not always helpful. Depending on the dataset, sometimes the will shift the distribution of the data and causing problem in inference \n",
    "## NOTE: if source data's gray scale is well controled, no need to scale and normalize\n",
    "\n",
    "data_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float, scale = False),\n",
    "    v2.Resize(size = 512, antialias=True,),\n",
    "])\n",
    "\n",
    "target_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.long, scale = False),\n",
    "    v2.Resize(size = 512, antialias=True,),\n",
    "    v2.Lambda(lambda x: torch.squeeze(x, dim = 0)),\n",
    "])\n",
    "\n",
    "weight_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float, scale = False),\n",
    "    v2.Resize(size = 512, antialias=True,),\n",
    "    v2.Lambda(lambda x: torch.squeeze(x, dim = 0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ffd517-3c07-44bf-bfe0-7368c6a5520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data set\n",
    "importlib.reload(DICHeLaSegDataset)\n",
    "\n",
    "color_categories = False\n",
    "\n",
    "trainvalidate_dataset = DICHeLaSegDataset.DICHeLaWeightedSegDataset(\n",
    "    data_image_path_globs = trainvalidate_data_file_path_globs,\n",
    "    seg_image_path_globs = trainvalidate_seg_file_path_globs,\n",
    "    seg_weight_path_globs = trainvalidate_weight_file_path_globs,\n",
    "    data_transform = data_transform,\n",
    "    target_transform = target_transform,\n",
    "    weight_transform = weight_transform,\n",
    "    common_transform = common_transform,\n",
    "    color_categories = color_categories,\n",
    ")\n",
    "\n",
    "print(f\"Tot data size = {len(trainvalidate_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4322f-9b5a-48c7-891d-a7ed703dd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train and validate dataset \n",
    "data_split_rand_genenrator = torch.Generator().manual_seed(0)\n",
    "data_split_ratios = [0.8, 0.2]\n",
    "\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(\n",
    "    trainvalidate_dataset, \n",
    "    data_split_ratios, \n",
    "    generator = data_split_rand_genenrator)\n",
    "\n",
    "print(f\"Train data size = {len(train_dataset)}\")\n",
    "print(f\"Validate data size = {len(validate_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41346ca3-6e60-4317-8ee1-e12d2ef1aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check data and label\n",
    "check_idx = 0\n",
    "\n",
    "check_dataset = train_dataset\n",
    "\n",
    "check_data, check_label, check_weight = check_dataset[check_idx]\n",
    "print(check_data.size())\n",
    "print(check_label.size())\n",
    "print(check_weight.size())\n",
    "    \n",
    "check_data = check_data.numpy()\n",
    "check_label = check_label.numpy()\n",
    "check_weight = check_weight.numpy()\n",
    "\n",
    "plt.figure(figsize = (7,2))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.rollaxis(check_data,0,3))\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Data\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(check_label, cmap = \"Set3\")\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Target\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(check_weight)\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Weight\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9313e2f-0c2a-4fbd-bdc1-024a0ca5051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data loader to training and validation dataset\n",
    "\n",
    "# NOTE: Use a very small batch size here to fit the data into my small GPU memory \n",
    "train_bath_size = 8\n",
    "validate_batch_size = 8\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size = train_bath_size, \n",
    "                                               shuffle = True)\n",
    "validate_dataloader = torch.utils.data.DataLoader(validate_dataset, \n",
    "                                               batch_size = validate_batch_size, \n",
    "                                               shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871f3ea-acc0-465c-bf16-612eb2ec31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "importlib.reload(UNets)\n",
    "\n",
    "in_channels = 1 # input image number of channels\n",
    "out_channels = 2 # output segmentation number of classes\n",
    "layer_nof_channels = [32, 64, 128, 256, 512]\n",
    "\n",
    "model = UNets.Simple3LayerUNet(\n",
    "    in_channels = in_channels,\n",
    "    out_channels = out_channels,\n",
    "    layer_nof_channels = layer_nof_channels,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e544c5b-dadd-4402-8754-aad3edbe2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use parallel computing if possible\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12a36a-e52d-4fdc-a649-b7bfcd9d79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## quickly check if model can run\n",
    "\n",
    "model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    check_features, check_labels, check_weight = next(iter(train_dataloader))\n",
    "    check_features = check_features.to(\"cpu\")\n",
    "    model.eval()\n",
    "    print(model(check_features).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11885f36-cd32-4644-b8df-8569909c8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training configuration\n",
    "importlib.reload(LossFunctions)\n",
    "\n",
    "loss_func = LossFunctions.WeightedCrossEntropyLoss(reduction = \"mean\")\n",
    "\n",
    "learning_rate = 2E-5\n",
    "nof_epochs = 400\n",
    "\n",
    "train_parameters = model.parameters()\n",
    "optimizer = torch.optim.Adam(train_parameters, lr = learning_rate)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer = optimizer,\n",
    "#     step_size = 80,\n",
    "#     gamma = 0.1,\n",
    "# )\n",
    "\n",
    "stop_lr = 1E-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359538d-17c7-4063-af50-04c74a58f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "importlib.reload(TrainAndValidate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "learning_rates = torch.zeros((nof_epochs,))\n",
    "train_losses = torch.zeros((nof_epochs,))\n",
    "validate_losses = torch.zeros((nof_epochs,))\n",
    "\n",
    "end_nof_epochs = 0\n",
    "\n",
    "for i_epoch in range(nof_epochs):\n",
    "    print(f\" ------ Epoch {i_epoch} ------ \")\n",
    "\n",
    "    end_nof_epochs = i_epoch\n",
    "    \n",
    "    cur_lr = optimizer.param_groups[0]['lr'];\n",
    "\n",
    "    if cur_lr < stop_lr:\n",
    "        break\n",
    "    \n",
    "    print(f\"current lr = {cur_lr}\")\n",
    "    learning_rates[i_epoch] = cur_lr\n",
    "\n",
    "    cur_train_loss = TrainAndValidate.train_one_epoch(\n",
    "        model = model,\n",
    "        train_loader = train_dataloader,\n",
    "        loss_func = loss_func,\n",
    "        optimizer = optimizer,\n",
    "        device = device,\n",
    "    )\n",
    "\n",
    "    cur_validate_loss = TrainAndValidate.validate_one_epoch(\n",
    "        model = model,\n",
    "        validate_loader = validate_dataloader,\n",
    "        loss_func = loss_func,\n",
    "        device = device,\n",
    "    )\n",
    "\n",
    "    # scheduler.step()\n",
    "    \n",
    "    train_losses[i_epoch] = cur_train_loss\n",
    "    validate_losses[i_epoch] = cur_validate_loss\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d73a4-1366-43cf-8ef8-03e1a175684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation metrics\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(train_losses[:end_nof_epochs], label = \"train loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(validate_losses[:end_nof_epochs], label = \"validation rate\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(learning_rates[:end_nof_epochs], label = \"learning rate\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11cb95-a420-4cc5-87ae-b870a44f18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check learning result\n",
    "check_idx =0\n",
    "check_batch_idx = 0\n",
    "check_dataloader = validate_dataloader\n",
    "\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    check_features = None\n",
    "    check_labels = None\n",
    "    \n",
    "    for i_batch in range(check_batch_idx + 1):\n",
    "        check_features, check_labels, check_weights = next(iter(check_dataloader))\n",
    "    \n",
    "    check_features = check_features.to(device)\n",
    "    check_preds = model(check_features)\n",
    "\n",
    "check_features = check_features.detach().cpu()\n",
    "check_preds = check_preds.detach().cpu()\n",
    "check_labels = check_labels.detach().cpu()\n",
    "\n",
    "check_preds = torch.argmax(check_preds, dim = 1)\n",
    "# check_preds = check_preds[1,...]\n",
    "\n",
    "\n",
    "check_feature = check_features[check_idx,...].numpy()\n",
    "check_pred = check_preds[check_idx,...].numpy()\n",
    "check_label = check_labels[check_idx,...].numpy()\n",
    "\n",
    "check_feature = np.rollaxis(check_feature,0,3)\n",
    "\n",
    "plt.figure(figsize = (7,2))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(check_feature)\n",
    "plt.title(\"input\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(check_pred)\n",
    "plt.colorbar()\n",
    "plt.title(\"prediction\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(check_label)\n",
    "plt.colorbar()\n",
    "plt.title(\"ground truth\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df97d28-ad32-4b3b-8fa3-5d3d865d6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model and model parameters\n",
    "\n",
    "dst_dir_path = r\".\\Results\"\n",
    "if not os.path.isdir(dst_dir_path):\n",
    "    os.makedirs(dst_dir_path)\n",
    "\n",
    "dst_model_name = \"model_\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "dst_model_file_name = dst_model_name + \".pt\"\n",
    "dst_modelstate_file_name = dst_model_name + \"_state.pt\"\n",
    "\n",
    "dst_model_file_path = os.path.join(dst_dir_path, dst_model_file_name)\n",
    "torch.save(model, dst_model_file_path)\n",
    "print(\"model saved to: \" + dst_model_file_path)\n",
    "\n",
    "dst_modelstate_file_path = os.path.join(dst_dir_path, dst_modelstate_file_name)\n",
    "torch.save(model.state_dict(), dst_modelstate_file_path)\n",
    "print(\"model state saved to: \" + dst_modelstate_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a601f-ddbf-4164-b898-30c60a7c0638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
