{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370b1d9-a779-46be-9c23-a4bf235d1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import time\n",
    "\n",
    "# adjust PyTorch parameter to enable more efficient use of GPU memory\n",
    "import os \n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:native, garbage_collection_threshold:0.6, max_split_size_mb:64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c49fe7-fa6a-4bf0-8925-57d3875c34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Modules.Models.UNets as UNets\n",
    "import Modules.Data.DICHeLaDataset as DICHeLaSegDataset \n",
    "import Modules.Data.ImageStackTransform as ImageStackTransform  \n",
    "import Modules.TrainAndValidate.TrainAndValidate as TrainAndValidate\n",
    "import Modules.TrainAndValidate.LossFunctions as LossFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a0d31-530c-4a5d-8d5a-7e8e9c7a102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalidate_data_file_path_globs = [\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01\\t*.tif\",\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02\\t*.tif\"\n",
    "]\n",
    "\n",
    "trainvalidate_seg_file_path_globs = [\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_ST\\SEG_ERODE\\man_seg*.tif\",\n",
    "    r\"E:\\Python\\DataSet\\TorchDataSet\\DIC-C2DH-HeLa\\Train\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_ST\\SEG_ERODE\\man_seg*.tif\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d3833-f6ab-4e69-805a-d3af8b90b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define data transforms\n",
    "importlib.reload(ImageStackTransform)\n",
    "\n",
    "# create common transforms\n",
    "common_transform = v2.Compose([\n",
    "    ImageStackTransform.ElasticTransform(fills = [\"mean\", \"min\", \"min\"], alpha = 50, sigma = 5),\n",
    "    ImageStackTransform.RandomRotation(fills = [\"mean\", \"min\", \"min\"], degrees = [-45, 45]),\n",
    "    ImageStackTransform.RandomCrop(\n",
    "        size = (256,256), \n",
    "        pad_if_needed = True, \n",
    "        padding_mode = \"reflect\",\n",
    "        \n",
    "    ),\n",
    "    ImageStackTransform.RandomHorizontalFlip(p = 0.5),\n",
    "    ImageStackTransform.RandomVerticalFlip(p = 0.5),\n",
    "    \n",
    "])\n",
    "\n",
    "## NOTE: scaling and normalization is not always helpful. Depending on the dataset, sometimes the will shift the distribution of the data and causing problem in inference \n",
    "## NOTE: if source data's gray scale is well controled, no need to scale and normalize\n",
    "\n",
    "data_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float, scale = False),\n",
    "    v2.Resize(size = 512, antialias=True,),\n",
    "])\n",
    "\n",
    "target_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.long, scale = False),\n",
    "    v2.Resize(size = 512, antialias=True,),\n",
    "    v2.Lambda(lambda x: torch.squeeze(x, dim = 0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a3883-ead3-4a35-961a-17005d0c3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data set\n",
    "importlib.reload(DICHeLaSegDataset)\n",
    "\n",
    "color_categories = False\n",
    "\n",
    "trainvalidate_dataset = DICHeLaSegDataset.DICHeLaSegDataset(\n",
    "    data_image_path_globs = trainvalidate_data_file_path_globs,\n",
    "    seg_image_path_globs = trainvalidate_seg_file_path_globs,\n",
    "    data_transform = data_transform,\n",
    "    target_transform = target_transform,\n",
    "    common_transform = common_transform,\n",
    "    color_categories = color_categories,\n",
    ")\n",
    "\n",
    "print(f\"Tot data size = {len(trainvalidate_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b835505-f694-462f-9a2b-e0818ef73708",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train and validate dataset \n",
    "data_split_rand_genenrator = torch.Generator().manual_seed(0)\n",
    "data_split_ratios = [0.8, 0.2]\n",
    "\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(\n",
    "    trainvalidate_dataset, \n",
    "    data_split_ratios, \n",
    "    generator = data_split_rand_genenrator)\n",
    "\n",
    "print(f\"Train data size = {len(train_dataset)}\")\n",
    "print(f\"Validate data size = {len(validate_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09804f-bd3c-4b29-80f4-97d562d52439",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check data and label\n",
    "check_idx = 6\n",
    "\n",
    "check_dataset = train_dataset\n",
    "\n",
    "check_data, check_label = check_dataset[check_idx]\n",
    "print(check_data.size())\n",
    "print(check_label.size())\n",
    "\n",
    "check_data = check_data.numpy()\n",
    "check_label = check_label.numpy()\n",
    "\n",
    "plt.figure(figsize = (7,2))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.rollaxis(check_data,0,3))\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Data\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(check_label, cmap = \"Set3\")\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Target\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18047787-a3f9-4a4c-b57a-3edddc58a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data loader to training and validation dataset\n",
    "\n",
    "# NOTE: Use a very small batch size here to fit the data into my small GPU memory \n",
    "train_bath_size = 8\n",
    "validate_batch_size = 8\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size = train_bath_size, \n",
    "                                               shuffle = True)\n",
    "validate_dataloader = torch.utils.data.DataLoader(validate_dataset, \n",
    "                                               batch_size = validate_batch_size, \n",
    "                                               shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14540ac4-24fb-4095-995c-3a73dd6de2fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load model\n",
    "importlib.reload(UNets)\n",
    "\n",
    "in_channels = 1 # input image number of channels\n",
    "out_channels = 2 # output segmentation number of classes\n",
    "layer_nof_channels = [32, 64, 128, 256, 512]\n",
    "\n",
    "model = UNets.Simple3LayerUNet(\n",
    "    in_channels = in_channels,\n",
    "    out_channels = out_channels,\n",
    "    layer_nof_channels = layer_nof_channels,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf8853-4e8e-4725-bd1a-f0194b316fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use parallel computing if possible\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91bbef-b93c-4618-8e07-e4be4bb5b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "## quickly check if model can run\n",
    "\n",
    "model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    check_features, check_labels = next(iter(train_dataloader))\n",
    "    check_features = check_features.to(\"cpu\")\n",
    "    model.eval()\n",
    "    print(model(check_features).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9e5c9-5605-4afa-8da9-a8110371a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training configuration\n",
    "importlib.reload(LossFunctions)\n",
    "\n",
    "loss_func = LossFunctions.CrossEntropyLoss(reduction = \"mean\")\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 2E-5\n",
    "nof_epochs = 400\n",
    "\n",
    "train_parameters = model.parameters()\n",
    "optimizer = torch.optim.Adam(train_parameters, lr = learning_rate)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer = optimizer,\n",
    "#     step_size = 80,\n",
    "#     gamma = 0.1,\n",
    "# )\n",
    "\n",
    "stop_lr = 1E-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93928daa-9b5f-4370-ac47-68a9dc174efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "importlib.reload(TrainAndValidate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "learning_rates = torch.zeros((nof_epochs,))\n",
    "train_losses = torch.zeros((nof_epochs,))\n",
    "validate_losses = torch.zeros((nof_epochs,))\n",
    "\n",
    "end_nof_epochs = 0\n",
    "\n",
    "for i_epoch in range(nof_epochs):\n",
    "    print(f\" ------ Epoch {i_epoch} ------ \")\n",
    "\n",
    "    end_nof_epochs = i_epoch\n",
    "    \n",
    "    cur_lr = optimizer.param_groups[0]['lr'];\n",
    "\n",
    "    if cur_lr < stop_lr:\n",
    "        break\n",
    "    \n",
    "    print(f\"current lr = {cur_lr}\")\n",
    "    learning_rates[i_epoch] = cur_lr\n",
    "\n",
    "    cur_train_loss = TrainAndValidate.train_one_epoch(\n",
    "        model = model,\n",
    "        train_loader = train_dataloader,\n",
    "        loss_func = loss_func,\n",
    "        optimizer = optimizer,\n",
    "        device = device,\n",
    "    )\n",
    "\n",
    "    cur_validate_loss = TrainAndValidate.validate_one_epoch(\n",
    "        model = model,\n",
    "        validate_loader = validate_dataloader,\n",
    "        loss_func = loss_func,\n",
    "        device = device,\n",
    "    )\n",
    "\n",
    "    # scheduler.step()\n",
    "    \n",
    "    train_losses[i_epoch] = cur_train_loss\n",
    "    validate_losses[i_epoch] = cur_validate_loss\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee3b2b-b65e-4a84-bac7-0d7a4e869a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation metrics\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(train_losses, label = \"train loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(validate_losses, label = \"validation rate\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(learning_rates, label = \"learning rate\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883531b-de81-4e8b-86a0-fabf8ee85aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check learning result\n",
    "check_idx =0\n",
    "check_batch_idx = 0\n",
    "check_dataloader = validate_dataloader\n",
    "\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    check_features = None\n",
    "    check_labels = None\n",
    "    \n",
    "    for i_batch in range(check_batch_idx + 1):\n",
    "        check_features, check_labels = next(iter(check_dataloader))\n",
    "    \n",
    "    check_features = check_features.to(device)\n",
    "    check_preds = model(check_features)\n",
    "\n",
    "check_features = check_features.detach().cpu()\n",
    "check_preds = check_preds.detach().cpu()\n",
    "check_labels = check_labels.detach().cpu()\n",
    "\n",
    "check_preds = torch.argmax(check_preds, dim = 1)\n",
    "# check_preds = check_preds[1,...]\n",
    "\n",
    "\n",
    "check_feature = check_features[check_idx,...].numpy()\n",
    "check_pred = check_preds[check_idx,...].numpy()\n",
    "check_label = check_labels[check_idx,...].numpy()\n",
    "\n",
    "check_feature = np.rollaxis(check_feature,0,3)\n",
    "\n",
    "plt.figure(figsize = (7,2))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(check_feature)\n",
    "plt.title(\"input\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(check_pred)\n",
    "plt.colorbar()\n",
    "plt.title(\"prediction\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(check_label)\n",
    "plt.colorbar()\n",
    "plt.title(\"ground truth\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e44996-7efc-46d6-8e35-c9e078623743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model and model parameters\n",
    "\n",
    "dst_dir_path = r\".\\Results\"\n",
    "if not os.path.isdir(dst_dir_path):\n",
    "    os.makedirs(dst_dir_path)\n",
    "\n",
    "dst_model_name = \"model_\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "dst_model_file_name = dst_model_name + \".pt\"\n",
    "dst_modelstate_file_name = dst_model_name + \"_state.pt\"\n",
    "\n",
    "dst_model_file_path = os.path.join(dst_dir_path, dst_model_file_name)\n",
    "torch.save(model, dst_model_file_path)\n",
    "print(\"model saved to: \" + dst_model_file_path)\n",
    "\n",
    "dst_modelstate_file_path = os.path.join(dst_dir_path, dst_modelstate_file_name)\n",
    "torch.save(model.state_dict(), dst_modelstate_file_path)\n",
    "print(\"model state saved to: \" + dst_modelstate_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b88723-03fa-4ec6-bd73-ff3d873cbdef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
